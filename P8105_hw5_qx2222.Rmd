---
title: "P8105_hw5_qx2222"
author: "Qiaoyi Xu"
date: "2022-11-15"
output: github_document
---


```{r}
library(tidyverse)
set.seed(1)
```

## Problem 1 (answer posted)

The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`. To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file. As a next step, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r, message=FALSE,warning=FALSE}
full_df = 
  tibble(
    files = list.files("data/problem1_data/"),
    path = str_c("data/problem1_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 




## Problem 2

### Import homicide data
```{r}
homicide = read_csv("data/homicide-data.csv") #import 'homicide' data
```


### Describe the raw data:

In the raw homicide data, there are `r nrow(homicide)` observations and `r ncol(homicide)` variables,such as `r names(homicide)`. This dataset describe data on more than 52,000 criminal homicides over the past decade in 50 of the largest American cities.


### Create new variables
```{r}
homicide = homicide %>%
  mutate(city_state = str_c(city, ", ", state)) %>%
  filter(city_state!="Tulsa, AL") %>% #clear error data: Tulsa is not located in AL
  mutate(homicide_status = if_else(disposition == "Closed without arrest", "unsolved",
                                   if_else(disposition == "Open/No arrest", "unsolved",
                                           if_else(disposition == "Closed by arrest", "solved", NA_character_))))


homicide
```

### Summarized table
```{r}
homicide_table = homicide %>%
  group_by(city_state) %>%
  summarise(total_homicide = n(),
            number_unsolved = sum(homicide_status == "unsolved")) %>%
  knitr::kable(col.names = c("City, State", "Total number of homicide", "Number of unsolved homicide"))

homicide_table
```

### For the city of Baltimore, MD
```{r}
Balto = homicide %>%
  filter(city_state == "Baltimore, MD") %>%
  summarise(total_homicide = n(),
            number_unsolved = sum(homicide_status == "unsolved"))

Balto_prop <- 
  prop.test(Balto %>% pull(number_unsolved), Balto %>% pull(total_homicide))

Balto_prop %>% broom::tidy() %>%
  knitr::kable()
```

### For each of the cities
```{r}
proptest_function = function(df) {
  summary = df %>% 
    summarise(total_homicide = n(),
              number_unsolved = sum(homicide_status == "unsolved"))
              
  cities_proptest = prop.test(summary %>% pull(number_unsolved), 
                              summary %>% pull(total_homicide)) %>%
    broom::tidy()
  
  cities_proptest
}


cities_final = homicide %>%
  nest(data = -city_state) %>%
  mutate(cities_test = map(data, proptest_function)) %>%
  unnest(cities_test) %>%
  select(city_state,estimate,starts_with('conf'))

cities_final %>%
  knitr::kable(col.names = c("City, State", "estimate","confidenceinterval_low","confidenceinterval_high"))
  
```

### Create plot (shows the estimates and 95% CIs for each city)
```{r}
plot_cities = cities_final %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes (x = city_state, y = estimate)) +
  geom_point()+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high))+
  theme(axis.text.x = element_text(angle = 90))+
  labs(x = "City, State", y = "Estimates of unsolved homicide porpotion",
       title = "Estimates and 95% CIs for each city")

plot_cities #shows the estimates and 95% CIs for each city


ggsave(
  filename = "results/plot for each city.pdf",
  plot = plot_cities,
  width = 30,
  height = 20,
  units = "cm"
  ) #export plot to 'results' directory

```

This plot shows the estimates and CIs for each city and organize cities according to the proportion of unsolved homicides.



## Problem 3

### First set the following design elements:
Fix n=30
Fix σ=5
Set μ=0. 

```{r}
#write t-test function
ttest_function = function(n = 30, sd = 5, mu = 0) {
  sample = rnorm(n = n, sd = sd, mean = mu)
  result = t.test(sample) %>%
  broom::tidy()%>%
  select(estimate, p.value)
  
  result
}


```

### Generate datasets use the function

```{r}
prob3_df = 
  expand.grid(mean = 0, iteration = 1:5000) %>%
  mutate(result = map(.x = mean, ~ttest_function(mu=.x))) %>%
  unnest(result) %>%
  knitr::kable()
  
```

### Repeat the above for μ={1,2,3,4,5,6}

```{r}
prob3_diffmean = 
  expand.grid(mean = 1:6, iteration = 1:5000) %>%
  mutate(result = map(.x = mean, ~ttest_function(mu=.x))) %>%
  unnest(result) 
```

### Make a plot(true mean v.s. proportion of times the null was rejected)

```{r}
mean_prop_plot = prob3_diffmean %>%
  group_by(mean) %>%
  summarize(prop_reject = sum(p.value<0.05) / 5000) %>%
  ggplot(aes(x = mean, y = prop_reject)) +
  geom_point()+
  geom_path() +
  scale_x_continuous(breaks = 1:6)+
  labs(x = "true mean", y = "proportion of times the null was rejected",
       title = " The true mean v.s. The power of the test")

mean_prop_plot

ggsave(
  filename = "results/mean v.s. rejected porportion.pdf",
  plot = mean_prop_plot,
  width = 30,
  height = 20,
  units = "cm"
  ) #export plot to 'results' directory
```

### Describe the association between effect size and power:

From the plot in output, we could see there is a increasing trend between the proportion of times the null was rejected and the true mean. Thus, there is a postive relationship between power and effect size. When the effect size increases, the power increases.


### Make a plot showing the average estimate of μ̂  on the y axis and the true value of μ on the x axis
```{r}
mean_estimate_plot = prob3_diffmean %>%
  group_by(mean) %>%
  summarize(average_estimate = mean(estimate)) %>%
  ggplot(aes(x = mean, y = average_estimate)) +
  geom_point()+
  geom_path() +
  scale_x_continuous(breaks = 1:6)+
  labs(x = "true mean", y = "the average estimate",
       title = "The true mean v.s. the average estimate")

mean_estimate_plot

ggsave(
  filename = "results/mean v.s. average estimate.pdf",
  plot = mean_estimate_plot,
  width = 30,
  height = 20,
  units = "cm"
  ) #export plot to 'results' directory
```

### Make a second plot (the average estimate of μ̂  only in samples for which the null was rejected on the y axis and the true value of μ on the x axis)

```{r}
rejected = prob3_diffmean %>%
  filter(p.value <= 0.05) %>%
  group_by(mean) %>%
  summarise(average_estimate = mean(estimate))

all_estimates = prob3_diffmean %>%
  group_by(mean) %>%
  summarise(average_estimate = mean(estimate))

overlay_plot = ggplot(all_estimates, aes(x = mean, y = average_estimate)) +
  geom_line(data = rejected, aes(color = "orange"))+
  geom_line(data = all_estimates, aes(color = "blue"))+
  geom_point(data = rejected, aes(color = "orange"))+
  geom_point(data = all_estimates, aes(color = "blue"))+
  scale_colour_manual(name = "",
                      values = c("orange" ="orange", "blue" = "blue"),
                      labels = c("All", "Rejected"))+
  labs(x = "true mean", y = "the average estimate",
       title = "The true mean v.s. the average estimate")
  
overlay_plot 

ggsave(
  filename = "results/the overlay plot.pdf",
  plot = overlay_plot,
  width = 30,
  height = 20,
  units = "cm"
  ) #export plot to 'results' directory
```


### Answer questions:
Is the sample average of μ̂  across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?

From the final overlay plot, we could conclude the sample average of μ̂  across tests for which the null is not approximately equal to the true value of μ. When the true mean is smaller than 4, the sample average estimates are very different with the true value. But then the true mean is between 4 and 6, the sample average estimates are more and more close to the true value. Because, when the effect size is small and the power is low, which makes fewer rejected samples and lower reject proportion. Thus, the average estimates are far away with the true value. On the contrary, when the effect size is big and the power is high, which makes more rejected samples and higher reject proportion. Thus, the average estimates are close to the true value.




